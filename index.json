[{"authors":null,"categories":null,"content":"I am a first year Ph.D. student in Computer Science at University of Minnesota. I work in Robotics:Perception and Manipulation (RPM) Lab with Professor Karthik Desingh. I am passionate about robotics, like vision, manipulation, planning, and perception. My current research topic is robot learning and manipulation with equivariant symmetric.\nBefore that, I worked as a research assistant in The Helping Hands Lab advised by Professor Robert Platt. Prior to my Ph.D. study, I received my Master‚Äôs degree in Robotics from Northeastern University and my Bacholor‚Äôs degree in Computer Engineering (Controls, Robotics, and Autonomy) at Virginia Tech and minored in Mathematics and Biomedical Engineering.\nI have more than three years experience in robotics mechanical design, embedded programming and software development. Willing to be a robotic scientist!\n","date":1607817600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1607817600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a first year Ph.D. student in Computer Science at University of Minnesota. I work in Robotics:Perception and Manipulation (RPM) Lab with Professor Karthik Desingh. I am passionate about robotics, like vision, manipulation, planning, and perception.","tags":null,"title":"GUANANG SU","type":"authors"},{"authors":["Xupeng Zhu","Dian Wang","Guanang Su","Ondrej Biza","Robin Walters","Robert Platt"],"categories":null,"content":" ","date":1688428800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688428800,"objectID":"ffdb8c03ad5a7e9ec5715d92526476ad","permalink":"https://xxs90.github.io/publication/ar_2023/","publishdate":"2023-07-04T00:00:00Z","relpermalink":"/publication/ar_2023/","section":"publication","summary":"Real-world grasp detection is challenging due to the stochasticity in grasp dynamics and the noise in hardware. Ideally, the system would adapt to the real world by training directly on physical systems. However, this is generally difficult due to the large amount of training data required by most grasp learning models. In this paper, we note that the planar grasp function is SE(2)-equivariant and demonstrate that this structure can be used to constrain the neural network used during learning. This creates an inductive bias that can significantly improve the sample efficiency of grasp learning and enable end-to-end training from scratch on a physical robot with as few as 600 grasp attempts. We call this method Symmetric Grasp learning (SymGrasp) and show that it can learn to grasp ‚Äúfrom scratch‚Äù in less that 1.5 h of physical robot time. This paper represents an expanded and revised version of the conference paper Zhu et al. (2022).","tags":["manipulation","learning"],"title":"On Robot Grasp Learning Using Equivariant Models","type":"publication"},{"authors":["Mingxi Jia","Dian Wang","Guanang Su","David Klee","Xupeng Zhu","Robin Walters","Robert Platt"],"categories":null,"content":" ","date":1685318400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685318400,"objectID":"0ef432fe9fef105a39f66d97811f69b8","permalink":"https://xxs90.github.io/publication/icra_2023/","publishdate":"2023-03-01T00:00:00Z","relpermalink":"/publication/icra_2023/","section":"publication","summary":"In robotic manipulation, acquiring samples is extremely expensive because it often requires interacting with the real world. Traditional image-level data augmentation has shown the potential to improve sample efficiency in various machine learning tasks. However, image-level data augmentation is insufficient for an imitation learning agent to learn good manipulation policies in a reasonable amount of demonstrations. We propose Simulation-augmented Equivariant Imitation Learning (SEIL), a method that combines a novel data augmentation strategy of supplementing expert trajectories with simulated transitions and an equivariant model that exploits the $O(2)$ symmetry in robotic manipulation. Experimental evaluations demonstrate that our method can learn non-trivial manipulation tasks within ten demonstrations and outperforms the baselines with a significant margin. **Project page with more details at https://saulbatman.github.io/project/seil/.**","tags":["manipulation","learning"],"title":"SEIL: Simulation-augmented Equivariant Imitation Learning","type":"publication"},{"authors":null,"categories":null,"content":" ","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"c29dd5e8a3c21609aaf6a12f318f8333","permalink":"https://xxs90.github.io/talk/self-introduction/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/talk/self-introduction/","section":"event","summary":"An introduction video of me.","tags":null,"title":"Self Introduction","type":"event"},{"authors":null,"categories":null,"content":"Introduction For navigation in robotics fields, especially autonomous driving tasks, accurate estimation of motion and 3D dynamic scene understanding are important for perception. Scene flow, a joint prediction method that comes up with the optical flow and stereo matching could faster solve the task and improves generalization by leveraging domain-specific information and avoiding bias.\nFor a traditional motion tracking pipeline, inputs the 3D data to do an object detection segmentation with segmenting into unique objects and then apply for the object tracking individually. Therefore, a segmentation error would lead to incorrect estimation in the future motion prediction for each object. Rather than directly applying to the data, scene flow estimation could compute a 3D motion vector for each point in the scene and remove reliance on the object.\nThis project proposed using point cloud data rather than directly estimating the 3D motion field from RGB images, followed by recent research.\n","date":1664582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664582400,"objectID":"17e9d8e4c95753b06f789f006c5a364b","permalink":"https://xxs90.github.io/project/sceneflow/","publishdate":"2022-10-01T00:00:00Z","relpermalink":"/project/sceneflow/","section":"project","summary":"An End-to-end scene flow estimation with PyTorch.","tags":["vision","learning"],"title":"Scene Flow Estimation for Autonomous Driving","type":"project"},{"authors":["Xupeng Zhu","Dian Wang","Ondrej Biza","Guanang Su","Robin Walters","Robert Platt"],"categories":null,"content":" ","date":165888e4,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":165888e4,"objectID":"ca2f35ded4fc48146867c293d3aa8ed2","permalink":"https://xxs90.github.io/publication/rss_2022/","publishdate":"2022-07-27T00:00:00Z","relpermalink":"/publication/rss_2022/","section":"publication","summary":"In planar grasp detection, the goal is to learn a function from an image of a scene onto a set of feasible grasp poses in SE(2). In this paper, we recognize that the optimal grasp function is SE(2)-equivariant and can be modeled using an equivariant convolutional neural network. As a result, we are able to significantly improve the sample efficiency of grasp learning, obtaining a good approximation of the grasp function after only 600 grasp attempts. This is few enough that we can learn to grasp completely on a physical robot in about 1.5 hours. **Project page with more details at https://zxp-s-works.github.io/equivariant_grasp_site/.**","tags":["manipulation","learning"],"title":"Sample Efficient Grasp Learning Using Equivariant Models","type":"publication"},{"authors":null,"categories":null,"content":"To achieve accurate robotics perceptual capacity, researchers typically use multiple sensors to obtain comprehensive and precise environmental information. These sensors are non-integrated, decentralized, and in most cases need to be configured separately. Researchers need to spend a great amount of time on the installation and configuration of these sensors. Meanwhile, the iPhone is a highly integrated electronic device with multiple precise sensors, but few researchers have used the iPhone as a hardware device for algorithm development because of the closed source properties of IOS.\nTo this end, we aim to develop an IOS-based ORBSLAM algorithm that has demonstrated the exploitability of the iPhone as well as providing a new possibilities of research platform. On another point, The advanced perception sensor on the iphone gives some imagination to the future development of the IOS-based metaverse app. By using the iPhone, many interesting applications can be further extended, such as augmented reality, indoor localization, etc.\nKeywords: Visual SLAM, Mobile device, Optimization\n","date":1648771200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648771200,"objectID":"90d5204c7c22e7f640b879e40a6ffe7a","permalink":"https://xxs90.github.io/project/orb-slam3/","publishdate":"2022-04-01T00:00:00Z","relpermalink":"/project/orb-slam3/","section":"project","summary":"ORB SLAM application on mobile device in real-time.","tags":["navigation","robotics"],"title":"ORB-SLAM3 on iPhone","type":"project"},{"authors":null,"categories":null,"content":" ","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"ade2f88b0edac1808c44faf49ada65f1","permalink":"https://xxs90.github.io/project/cyclegan/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/project/cyclegan/","section":"project","summary":"Learn the mapping between two unpaired images and apply the image translation in between.","tags":["learning","vision"],"title":"Images Translation with CycleGAN","type":"project"},{"authors":null,"categories":null,"content":" ","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"96d3aaeac37c7799be63163e492c9dd2","permalink":"https://xxs90.github.io/project/her/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/project/her/","section":"project","summary":" ","tags":["learning","manipulation","robotics"],"title":"Improving Sample Efficiency of Robot Grasping","type":"project"},{"authors":null,"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"d1cb43a65740d3743ef889e784268610","permalink":"https://xxs90.github.io/project/sharkpulse/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/project/sharkpulse/","section":"project","summary":"Learn the mapping between two unpaired images and apply the image translation in between.","tags":["learning","vision"],"title":"SharkPulse - Shark Image Identification, Dataset Building, Machine Learning","type":"project"},{"authors":["GUANANG SU","Âê≥ÊÅ©ÈÅî"],"categories":["Demo","ÊïôÁ®ã"],"content":"Overview The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It‚Äôs a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more Get Started üëâ Create a new site üìö Personalize your site üí¨ Chat with the Wowchemy community or Hugo community üê¶ Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy üí° Request a feature or report a bug for Wowchemy ‚¨ÜÔ∏è Updating Wowchemy? View the Update Tutorial and Release Notes Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n‚ù§Ô∏è Click here to become a sponsor and help support Wowchemy‚Äôs future ‚ù§Ô∏è As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features ü¶Ñ‚ú®\nEcosystem Hugo Academic CLI: Automatically import publications from BibTeX Inspiration Check out the latest demo of what you‚Äôll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, ‰∏≠Êñá, and Portugu√™s Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files. Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"fa567a01989e2a2b0726c9c08c13bd83","permalink":"https://xxs90.github.io/post/self-intro/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/self-intro/","section":"post","summary":"Welcome üëã We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","ÂºÄÊ∫ê"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let‚Äôs make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://xxs90.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"}]