---
title: ORB-SLAM3 on iPhone
summary: ORB SLAM application on mobile device in real-time.
tags:
  - navigation
  - robotics
date: '2022-04-01'

# Optional external URL for project (replaces project detail page).
external_link: ''

image:
  # caption: Photo by rawpixel on Unsplash
  # focal_point: Smart

# links:
#   - icon: twitter
#     icon_pack: fab
#     name: Follow
#     url: https://twitter.com/georgecushen
url_code: 'https://gitlab.com/xxs000/eece5554/-/tree/main/final_project'
url_pdf: ''
url_slides: 'uploads/RSN_final_presentation.pdf'
url_video: ''

# Slides (optional).
#   Associate this project with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides: ""
---

To achieve accurate robotics perceptual capacity, researchers typically use multiple sensors to obtain comprehensive and precise environmental information. These sensors are non-integrated, decentralized, and in most cases need to be configured separately. Researchers need to spend a great amount of time on the installation and configuration of these sensors. Meanwhile, the iPhone is a highly integrated electronic device with multiple precise sensors, but few researchers have used the iPhone as a hardware device for algorithm development because of the closed source properties of IOS.

To this end, we aim to develop an IOS-based ORBSLAM algorithm that has demonstrated the exploitability of the iPhone as well as providing a new possibilities of research platform. On another point, The advanced perception sensor on the iphone gives some imagination to the future development of the IOS-based metaverse app. By using the iPhone, many interesting applications can be further extended, such as augmented reality, indoor localization, etc.

Keywords: Visual SLAM, Mobile device, Optimization
